{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clone code t·ª´ github v√† chu·∫©n b·ªã m√¥i tr∆∞·ªùng"
      ],
      "metadata": {
        "id": "-oxcsWOgiu2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX5ZCS1BaktF"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/duybui1911/AI_CHALLENGE_sample.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AI_CHALLENGE_sample"
      ],
      "metadata": {
        "id": "3minkioY47iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### C√†i c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt n√®, c√†i m·∫•t t·∫ßm 5m g√¨ ƒë√≥. ƒê·∫∑t ƒë√¢y r·ªìi ƒëi pha c·ªëc coffee xong l√† v·ª´a :V"
      ],
      "metadata": {
        "id": "k61tM7mMqPaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "metadata": {
        "id": "qpOYW3LnanR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from threading import Thread\n",
        "from typing import Iterator\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
        "\n",
        "from tools.utils import use_calculator\n",
        "from dataset import get_data, TestDataset"
      ],
      "metadata": {
        "id": "tMCfd6Md4uNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Load model t·ª´ huggingface, nh∆∞ c√°c b·∫°n th·∫•y ·ªü h√¨nh d∆∞·ªõi ƒë√¢y th√¨ ƒë√¢y l√† 1 m√¥ h√¨nh m√¨nh th·∫•y kh√° ·ªïn so v·ªõi c√°c model ƒë·ªìng trang l·ª©a =))))\n",
        "<img src = https://raw.githubusercontent.com/nlpxucan/WizardLM/main/WizardMath/images/wizardmath_gsm8k.png>\n",
        "### L∆∞u √Ω 1: tr∆∞·ªõc khi load model th√¨ n√™n ƒëƒÉng nh·∫≠p v√¥ huggingface tr∆∞·ªõc cho ƒë·ª° l·ªói nha c√°c bro :v\n",
        "### Link ƒëƒÉng nh·∫≠p: <https://huggingface.co/login>\n",
        "\n",
        "### L∆∞u √Ω 2: Do m√¨nh so·∫°n h∆∞·ªõng d·∫´n n√†y v·ªõi b·∫£n colab d√πng ch√πa n√™n b·ªã gi·ªõi h·∫°n nhi·ªÅu v·ªÅ VRAM, v√¨ v·∫≠y m√¨nh load model ch·ªâ v·ªõi float16. ƒêi·ªÅu n√†y s·∫Ω l√†m gi·∫£m t√≠nh ch√≠nh x√°c 1 ch√∫t c·ªßa m√¥ h√¨nh, n·∫øu b·∫°n n√†o n·∫°p VJP th√¨ c√≥ th·ªÉ ƒë·ªÉ l√™n float32 or float64 nh√© üßõ\n",
        "\n",
        "\n",
        "### L∆∞u √Ω 3: Load model l√¢u vch n√™n l·∫ßn n√†y u·ªëng xong coffee th√¨ ki·∫øm ch·ªó n√†o bu√¥n chuy·ªán t·∫ßm 10m r·ªìi quay l·∫°i :v"
      ],
      "metadata": {
        "id": "1Cjm-zkYjCfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'WizardLM/WizardMath-7B-V1.0'\n",
        "if torch.cuda.is_available():\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map='auto'\n",
        "    )\n",
        "else:\n",
        "    model = None\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "Lqj_z07qQv0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sau khi load model th√†nh c√¥ng th√¨ ch·∫°y c√°c h√†m ƒë·ªÉ g·ªçi k·∫øt qu·∫£ nh∆∞ sau"
      ],
      "metadata": {
        "id": "A9811mRlj2oq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### H√†m config prompt cho model"
      ],
      "metadata": {
        "id": "DvizB83qkKW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prompt(message: str, chat_history: list[tuple[str, str]],\n",
        "               system_prompt: str) -> str:\n",
        "    texts = [f'[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n']\n",
        "    #for user_input, response in chat_history:\n",
        "    #    texts.append(f'{user_input.strip()} [/INST] {response.strip()}[INST] ')\n",
        "    texts.append(f'{message.strip()} [/INST]')\n",
        "    return ''.join(texts)"
      ],
      "metadata": {
        "id": "FhC0hitdkR4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### H√†m th·ª±c thi ch√≠nh khi run model"
      ],
      "metadata": {
        "id": "NkzzsDf6kSKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run(message: str,\n",
        "        chat_history: list[tuple[str, str]],\n",
        "        system_prompt: str,\n",
        "        max_new_tokens: int = 100,\n",
        "        temperature: float = 0.5,\n",
        "        top_p: float = 0.95,\n",
        "        top_k: int = 50) -> Iterator[str]:\n",
        "    prompt = get_prompt(message, chat_history, system_prompt)\n",
        "    inputs = tokenizer([prompt], return_tensors='pt').to(\"cuda\")\n",
        "\n",
        "    streamer = TextIteratorStreamer(tokenizer,\n",
        "                                    timeout=10.,\n",
        "                                    skip_prompt=True,\n",
        "                                    skip_special_tokens=True)\n",
        "    generate_kwargs = dict(\n",
        "        inputs,\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        temperature=temperature,\n",
        "        num_beams=1,\n",
        "    )\n",
        "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
        "    t.start()\n",
        "\n",
        "    outputs = []\n",
        "    for text in streamer:\n",
        "        outputs.append(text)\n",
        "        yield ''.join(outputs)"
      ],
      "metadata": {
        "id": "FsBDk3XskZ2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### H√†m g·ªçi v√† gen k·∫øt qu·∫£"
      ],
      "metadata": {
        "id": "cwUvC5mUkk2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(\n",
        "    message: str,\n",
        "    history_with_input: list[tuple[str, str]],\n",
        "    system_prompt: str,\n",
        "    max_new_tokens: int,\n",
        "    top_p: float,\n",
        "    temperature: float,\n",
        "    top_k: int,\n",
        ") -> Iterator[list[tuple[str, str]]]:\n",
        "\n",
        "    history = history_with_input[:-1]\n",
        "    generator = run(message, history, system_prompt, max_new_tokens,\n",
        "                    temperature, top_p, top_k)\n",
        "    try:\n",
        "        first_response = next(generator)\n",
        "        yield history + [(message, first_response)]\n",
        "    except StopIteration:\n",
        "        yield history + [(message, '')]\n",
        "    for response in generator:\n",
        "        yield history + [(message, response)]"
      ],
      "metadata": {
        "id": "npcwlc-YWn8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_example(message: str,\n",
        "                    system_prompt: str,\n",
        "                    max_new_tokens: int,\n",
        "                    top_p: float,\n",
        "                    temperature: float,\n",
        "                    top_k: int,) -> tuple[str, list[tuple[str, str]]]:\n",
        "    generator = generate(\n",
        "                          message= message,\n",
        "                          history_with_input= [],\n",
        "                          system_prompt= system_prompt,\n",
        "                          max_new_tokens= max_new_tokens,\n",
        "                          top_p= top_p,\n",
        "                          temperature= temperature,\n",
        "                          top_k= top_k,\n",
        "                          )\n",
        "    for x in generator:\n",
        "        pass\n",
        "    return '', x"
      ],
      "metadata": {
        "id": "lHkEZhK7WrLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### H√†m convert t·ª´ k·∫øt qu·∫£ model gen ra sang ƒë·ªãnh d·∫°ng file n·ªôp"
      ],
      "metadata": {
        "id": "TO_xPt1Sk8Q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input d·∫°ng: 'The answer is: 80.' v√† chu·ªói options\n",
        "# Output s·∫Ω l√† ƒë√°p √°n ch·ª©a 80 ho·∫∑c 80 n·∫øu options = \"\"\n",
        "import re\n",
        "def convert_to_submit_file(api_result: str = '', options: str = ''):\n",
        "    api_result = ((api_result.replace('/s', '')).replace('\\n', '')).replace(' ', '')\n",
        "    api_result = re.sub(r'[^a-zA-Z0-9.:\\\\)]', '', api_result)\n",
        "    answer_start = api_result.lower().find(\":\")\n",
        "    if answer_start != -1:\n",
        "        if api_result[-1] == '.':\n",
        "            api_result = api_result[:-1]\n",
        "        answer_part = api_result[answer_start + 1:].strip()\n",
        "        if any(c.isalpha() for c in answer_part):\n",
        "            answer = answer_part[0:answer_part.find(\")\")]\n",
        "            answer =  answer.lower()\n",
        "\n",
        "        else:\n",
        "            answer = answer_part.lower()\n",
        "\n",
        "        if options != '':\n",
        "            options = options.lower().replace(\" \", \"\")\n",
        "            if is_number(answer):\n",
        "              answer_id = options.find(str(answer))\n",
        "              character_id = options.rfind(')', 0, answer_id)\n",
        "              answer = options[character_id-1]\n",
        "            else:\n",
        "              answer = answer\n",
        "            #for option in options_lower.split(','):\n",
        "            #    if ' ' + option.strip() + ' ' in api_result.lower():\n",
        "            #        return option.strip()\n",
        "        return answer\n",
        "\n",
        "    return 'Nan'\n",
        "\n",
        "def is_number(s):\n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False"
      ],
      "metadata": {
        "id": "KmoH6j423klE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### S·ª≠a link d·ªØ li·ªáu ·ªü ƒë√¢y nh√©, tuy nhi√™n m√¨nh khuy√™n n√™n d√πng lu√¥n folder data v√† n·∫øu s·ª≠ d·ª•ng ·∫£nh th√¨ upload lu√¥n l√™n ƒë√≥.\n",
        "\n",
        "#### L·∫°i 1 c√°i l∆∞u √Ω n·ªØa: Do model n·∫∑ng r·ªìi v·ªõi c·∫£ colab kh√¥ng cho load file json qu√° n·∫∑ng ƒë√¢u. N√™n c√°c b·∫°n n√™n ch·∫°y s·ªë l∆∞·ª£ng c√¢u nh·ªè h∆°n **30K** d√≤ng nh√©. Nh∆∞ th·∫ø th√¨ file test s·∫Ω ph·∫£i chia ra l√†m 3 file r·ªìi khi xong th√¨ n·ªëi c√°c file v√†o 1 ƒë·ªÉ submit, m√¨nh c≈©ng chia lu√¥n cho c√°c b·∫°n r·ªìi ƒë·∫•y ‚úå\n",
        "\n",
        "#### **L∆∞u √Ω cu·ªëi c√πng 1:** Sau khi ch·∫°y xong file test n√†o th√¨ download lu√¥n file k·∫øt qu·∫£ ƒë√≥ v·ªÅ r·ªìi nh√©, ƒë·ªÅ ph√≤ng tr∆∞·ªùng h·ª£p c√°c b·∫°n may m·∫Øn b·ªã anh gg ng·∫Øt gi·ªØa ch·ª´ng th√¨ bay m·∫•t g√≥i coffee  =)))))))))\n",
        "#### File k·∫øt qu·∫£ t∆∞∆°ng ·ª©ng l√†: results1.txt cho test1.json, ..., n·ªëi file results1.txt, results2.txt, results2.txt l·∫ßn l∆∞·ª£t t·ª´ tr√™n xu·ªëng th√†nh 1 file t·ªïng results.txt xong zip l·∫°i l√† okii r√πi"
      ],
      "metadata": {
        "id": "K-ZWR-hZqmZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_link = './data/'"
      ],
      "metadata": {
        "id": "wurmz3rSkZ5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_names = ['test1.json', 'test2.json', 'test3.json']"
      ],
      "metadata": {
        "id": "JUKhAsFUpvhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "for file_id, file_name in enumerate(file_names):\n",
        "    submit_list = []\n",
        "    test_examples = get_data(os.path.join(data_link, file_name))\n",
        "    with open(f'./results/results{file_id+1}.txt', 'w') as f:\n",
        "        for problem in test_examples:\n",
        "            # Thay ƒë·ªïi prompt c·ªßa c√°c b·∫°n ·ªü ƒë√¢y, n√™n t·∫°o prompt h∆∞·ªõng v·∫´n v√† chia nh·ªè c√°c ch·ªß ƒë·ªÅ ƒë·ªÉ\n",
        "            # k·∫øt qu·∫£ ƒë∆∞·ª£c t·ªët h∆°n.\n",
        "            # L∆∞u √Ω: M√¨nh ch∆∞a x·ª≠ l√Ω ph·∫ßn ·∫£nh, n√™n c√°c b·∫°n c√≥ th·ªÉ x·ª≠ l√Ω th√™m v√†o ph·∫ßn n√†y r·ªìi n·ªëi v√†o prompt ho·∫∑c ques\n",
        "\n",
        "            prompt = \"Help me choose the correct answer to the following problem.\"\n",
        "            if problem[\"options\"] != \"\":\n",
        "                prompt += \" Note: you only need to provide the letter that precedes the correct answer. For example: a). Here is the question:\"\n",
        "            else:\n",
        "              prompt += \" Note: you only need to give the correct answer. For example: 10. Here is the question:\"\n",
        "            ques = '\\nQuestion:' + problem[\"Problem\"]\n",
        "            max_len = 15\n",
        "            sys_rq = prompt + ques\n",
        "            try:\n",
        "                start = time.time()\n",
        "                answer = process_example(\n",
        "                        message= ques,  # C√°i n√†y l√† c√¢u h·ªèi v√† options\n",
        "                        system_prompt= prompt, # C√°i n√†y l√† h∆∞·ªõng d·∫´n n√≥\n",
        "                        max_new_tokens = max_len, # ƒê√¢y l√† ƒë·ªô d√†i t·ªëi ƒëa c·ªßa ph·∫ßn model tr·∫£ v·ªÅ\n",
        "                        top_p= 0.9, # 3 tham s·ªë n√†y c√°c b·∫°n xem l·∫°i bu·ªïi 2 m√¨nh c√≥ n√≥i r·ªìi nh√©\n",
        "                        temperature= 0.5,\n",
        "                        top_k= 200\n",
        "                )\n",
        "                _, answer = answer[1][0]\n",
        "                end = time.time()\n",
        "                submit_item = convert_to_submit_file(answer, problem[\"options\"])\n",
        "                submit_list.append(submit_item)\n",
        "            except Exception as e:\n",
        "                submit_list.append('Nan')\n",
        "                print(e)\n",
        "            print(f\"Test {file_id+1}\\tTime {problem['id']}: {end-start}, answer: {submit_item}\")\n",
        "            f.write(submit_list[-1] + '\\t' + str(end-start) + '\\n')\n",
        "    print(f'\\n-------------------------------------------\\nEND test{file_id+1}.json\\n-------------------------------------------\\n')\n"
      ],
      "metadata": {
        "id": "Ih8pl1d6Gwfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **L∆∞u √Ω cu·ªëi c√πng 2:** Thi tho·∫£ng x√≥a c√°i n√≥ ch·∫°y gen ra m√†n h√¨nh ƒëi v√† thi tho·∫£ng t∆∞∆°ng t√°c v·ªõi giao di·ªán c·ªßa colab c√°i kh√¥ng b·ªè l√¢u n√≥ d·ªói n√≥ ng·∫Øt k·∫øt n·ªëi l√† ƒëi tong ƒë·∫•y. Ny m√† m√¨nh b∆° c√≤n b·ªã ƒë√° n·ªØa l√† ƒëang d√πng ch√πa :v  "
      ],
      "metadata": {
        "id": "zH5_mvrZUIUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ch√∫c c√°c b·∫°n ch·∫°y kh√¥ng bug v√† ƒë∆∞·ª£t k·∫øt qu·∫£ t·ªët :v"
      ],
      "metadata": {
        "id": "8yhXm_jDPzYE"
      }
    }
  ]
}